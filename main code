import requests
from bs4 import BeautifulSoup
import pandas as pd

def get_project_links():
    url = "https://hprera.nic.in/PublicDashboard"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    
    registered_projects_section = soup.find_all("div", {"class": "card-body"})[1]
    project_links = registered_projects_section.find_all("a", href=True)
    
    links = []
    for link in project_links[:6]:
        links.append("https://hprera.nic.in" + link['href'])
    
    return links

def get_project_details(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    
    details = {}
    try:
        details['GSTIN No'] = soup.find("span", {"id": "ContentPlaceHolder1_lblGST"}).text.strip()
    except AttributeError:
        details['GSTIN No'] = "N/A"
    try:
        details['PAN No'] = soup.find("span", {"id": "ContentPlaceHolder1_lblPan"}).text.strip()
    except AttributeError:
        details['PAN No'] = "N/A"
    try:
        details['Name'] = soup.find("span", {"id": "ContentPlaceHolder1_lblPromoterName"}).text.strip()
    except AttributeError:
        details['Name'] = "N/A"
    try:
        details['Permanent Address'] = soup.find("span", {"id": "ContentPlaceHolder1_lblPromoterAddress"}).text.strip()
    except AttributeError:
        details['Permanent Address'] = "N/A"
    
    return details

def main():
    project_links = get_project_links()
    project_data = []

    for link in project_links:
        details = get_project_details(link)
        project_data.append(details)
    
    df = pd.DataFrame(project_data)
    df.to_csv("project_details.csv", index=False)
    print("Scraping completed. Details saved to project_details.csv")

if __name__ == "__main__":
    main()
